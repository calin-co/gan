{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/calin/anaconda3/envs/mlp/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "mnist = input_data.read_data_sets('../../MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(input):\n",
    "    '''\n",
    "    input = [28*28, batch_size] array\n",
    "    return the prob of being real \n",
    "    '''\n",
    "    h0 = tf.layers.dense(input, 28*28, activation = tf.nn.relu, name='d0')\n",
    "    h1 = tf.layers.dense(h0, 1,  name='d1')\n",
    "    prob = tf.sigmoid(h1, 'd2')\n",
    "    \n",
    "    return prob\n",
    "\n",
    "def generator(inputs):\n",
    "    '''\n",
    "    receives random noise and tries to reproduce mnist image\n",
    "    '''\n",
    "    h0 = tf.layers.dense(inputs, 100, activation=tf.nn.relu, name='g0')\n",
    "    h1 = tf.layers.dense(h0, 28*28, name='g1')\n",
    "    h2 = tf.sigmoid(h1, name='g2')\n",
    "    return h2\n",
    "\n",
    "\n",
    "with tf.variable_scope('G'):\n",
    "    z = tf.placeholder(tf.float32, shape=[None, 100], name='z')\n",
    "    fake_samples = generator(z)\n",
    "   \n",
    "    \n",
    "with tf.variable_scope('D') as scope:\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784], name='inputs')\n",
    "    D_real = discriminator(x)\n",
    "    scope.reuse_variables()\n",
    "    D_fake = discriminator(fake_samples)\n",
    "    \n",
    "\n",
    "vars = tf.trainable_variables()\n",
    "d_params = [v for v in vars if v.name.startswith('D/')]\n",
    "g_params = [v for v in vars if v.name.startswith('G/')]\n",
    "\n",
    "'''\n",
    "loss_d = - tf.reduce_mean(tf.log(D1) + tf.log(1.0 - D2))\n",
    "loss_g = - tf.reduce_mean(tf.log(D2))\n",
    "''' \n",
    "\n",
    "G_loss = -tf.reduce_mean(D_fake)\n",
    "D_loss = tf.reduce_mean(D_fake) - tf.reduce_mean(D_real)\n",
    "\n",
    "epsilon = tf.random_uniform(\n",
    "    shape=[batch_size, 1, 1, 1],\n",
    "    minval=0.,\n",
    "    maxval=1.\n",
    ")\n",
    "\n",
    "X_hat = X_real + epsilon * (X_fake - X_real)\n",
    "D_X_hat = D(X_hat, reuse=True)\n",
    "grad_D_X_hat = tf.gradients(D_X_hat, [X_hat])[0]\n",
    "\n",
    "red_idx = [-1]\n",
    "\n",
    "slopes = tf.sqrt(tf.reduce_sum(tf.square(grad_D_X_hat), reduction_indices=red_idx))\n",
    "gradient_penalty = tf.reduce_mean((slopes - 1.)**2)\n",
    "D_loss += 10 * gradient_penalty\n",
    "\n",
    "\n",
    "d_optimizer = tf.train.AdamOptimizer().minimize(loss_d, var_list=d_params)\n",
    "g_optimizer = tf.train.AdamOptimizer().minimize(loss_g, var_list=g_params)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 100000\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            save_path = saver.save(sess, \"/tmp/model.ckpt\")\n",
    "\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            fake_data = sess.run(fake_samples, feed_dict={z: sample_Z(16, 100)})\n",
    "            fig = plot(fake_data)\n",
    "            plt.savefig('out/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "\n",
    "        \n",
    "        train_data, _ = mnist.train.next_batch(batch_size)\n",
    "        samples = sample_Z(batch_size, 100)\n",
    "\n",
    "        #update discriminator\n",
    "        loss_disc, _ = sess.run([loss_d, d_optimizer], {\n",
    "            x: train_data,\n",
    "            z: samples\n",
    "        })\n",
    "\n",
    "        #update generator\n",
    "        loss_gen, _ = sess.run([loss_g, g_optimizer], {\n",
    "            z: sample_Z(batch_size, 100)\n",
    "        })\n",
    "        \n",
    "        loss_gen, _ = sess.run([loss_g, g_optimizer], {\n",
    "            z: sample_Z(batch_size, 100)\n",
    "        })\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(loss_disc, loss_gen)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vanilla gan\n",
    "\"\"\"\n",
    "def discriminator(input):\n",
    "    '''\n",
    "    input = [28*28, batch_size] array\n",
    "    return the prob of being real \n",
    "    '''\n",
    "    h0 = tf.layers.dense(input, 28*28, activation = tf.nn.relu, name='d0')\n",
    "    h1 = tf.layers.dense(h0, 1,  name='d1')\n",
    "    prob = tf.sigmoid(h1, 'd2')\n",
    "    \n",
    "    return prob\n",
    "\n",
    "def generator(inputs):\n",
    "    '''\n",
    "    receives random noise and tries to reproduce mnist image\n",
    "    '''\n",
    "    h0 = tf.layers.dense(inputs, 100, activation=tf.nn.relu, name='g0')\n",
    "    h1 = tf.layers.dense(h0, 28*28, name='g1')\n",
    "    h2 = tf.sigmoid(h1, name='g2')\n",
    "    return h2\n",
    "\n",
    "\n",
    "with tf.variable_scope('G'):\n",
    "    z = tf.placeholder(tf.float32, shape=[None, 100], name='z')\n",
    "    fake_samples = generator(z)\n",
    "   \n",
    "    \n",
    "with tf.variable_scope('D') as scope:\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784], name='inputs')\n",
    "    D1 = discriminator(x)\n",
    "    scope.reuse_variables()\n",
    "    D2 = discriminator(fake_samples)\n",
    "    \n",
    "\n",
    "vars = tf.trainable_variables()\n",
    "d_params = [v for v in vars if v.name.startswith('D/')]\n",
    "g_params = [v for v in vars if v.name.startswith('G/')]\n",
    "\n",
    "loss_d = - tf.reduce_mean(tf.log(D1) + tf.log(1.0 - D2))\n",
    "loss_g = - tf.reduce_mean(tf.log(D2))\n",
    "    \n",
    "d_optimizer = tf.train.AdamOptimizer().minimize(loss_d, var_list=d_params)\n",
    "g_optimizer = tf.train.AdamOptimizer().minimize(loss_g, var_list=g_params)\n",
    "    \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
